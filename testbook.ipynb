{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torchvision import models, transforms\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "\n",
    "import object3d\n",
    "from models.MeshNet import MeshNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNet40WithImage(torch.utils.data.Dataset):\n",
    "    def __init__(self, cfg, renderer, mode='train'):\n",
    "        self.root = cfg['data_root']\n",
    "        self.augmentation = cfg['augmentation']\n",
    "        self.mode = mode\n",
    "\n",
    "        self.data = []\n",
    "        labels = os.listdir(self.root)\n",
    "        labels.sort()\n",
    "        for label_index, label in enumerate(labels):\n",
    "            type_root = os.path.join(self.root, label, mode)\n",
    "            for filename in os.listdir(type_root):\n",
    "                if filename.endswith('.npz'):\n",
    "                    identity = os.path.join(type_root, filename[:-4])\n",
    "                    self.data.append((identity, label_index))\n",
    "                    \n",
    "        # init renderer\n",
    "        self.renderer = renderer\n",
    "        \n",
    "        # resnet transform\n",
    "        if mode == 'train':\n",
    "            self.resnet_transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.resnet_transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        identity, label = self.data[i]\n",
    "        \n",
    "        # generate 2D Image\n",
    "        self.renderer.set_obj(identity + '.off')\n",
    "        self.renderer.random_context(coverage=0.8, obj_color=True, obj_translate=False, obj_rotation=True)\n",
    "        pil_image = self.renderer.render(binary=False).convert('RGB')\n",
    "        torch_image = self.resnet_transform(pil_image)\n",
    "        \n",
    "        # process meshnet data\n",
    "        data = np.load(identity + '.npz')\n",
    "        centers = data['centers'] # [face, 3]\n",
    "        corners = data['corners'] # [face, vertice, 3]\n",
    "        normals = data['normals'] # [face, 3]\n",
    "        neighbors_index = data['neighbors_index'] # [face, ?]\n",
    "        \n",
    "        num_point = len(centers)\n",
    "        # fill for n < 1024\n",
    "        if num_point < 1024:\n",
    "            chosen_indexes = np.random.randint(0, num_point, size=(1024 - num_point))\n",
    "            centers = np.concatenate((centers, centers[chosen_indexes]))\n",
    "            corners = np.concatenate((corners, corners[chosen_indexes]))\n",
    "            normals = np.concatenate((normals, normals[chosen_indexes]))\n",
    "            neighbors_index = np.concatenate((neighbors_index, neighbors_index[chosen_indexes]))\n",
    "            \n",
    "            # choose 3 neighbors\n",
    "            new_neighbors_index = np.empty([1024, 3], dtype=np.int64)\n",
    "            for idx in range(1024):\n",
    "                neighbors = neighbors_index[idx]\n",
    "                if len(neighbors) > 3:\n",
    "                    new_neighbors_index[idx] = np.random.choice(neighbors, 3, replace=False)\n",
    "                else:\n",
    "                    new_neighbors_index[idx] = np.concatenate((neighbors, [idx]*(3-len(neighbors))))\n",
    "\n",
    "            neighbors_index = new_neighbors_index\n",
    "        else:\n",
    "            chosen_indexes = np.random.choice(num_point, size=1024, replace=False)\n",
    "            centers = centers[chosen_indexes]\n",
    "            corners = corners[chosen_indexes]\n",
    "            normals = normals[chosen_indexes]\n",
    "            neighbors_index = neighbors_index[chosen_indexes]\n",
    "            # remove unlinkable index and choose 3 neighbors\n",
    "            new_neighbors_index = np.empty([1024, 3], dtype=np.int64)\n",
    "            for idx in range(1024):\n",
    "                mask = np.in1d(neighbors_index[idx], chosen_indexes)\n",
    "                neighbors = np.array(neighbors_index[idx])[mask]\n",
    "                if len(neighbors) > 3:\n",
    "                    new_neighbors_index[idx] = np.random.choice(neighbors, 3, replace=False)\n",
    "                else:\n",
    "                    new_neighbors_index[idx] = np.concatenate((neighbors, [chosen_indexes[idx]]*(3-len(neighbors))))\n",
    "\n",
    "            # re-index the neighbor\n",
    "            invert_index = {value: key for key, value in enumerate(chosen_indexes)}\n",
    "            neighbors_index = np.vectorize(invert_index.get, cache=True)(new_neighbors_index)\n",
    "\n",
    "        # data augmentation\n",
    "        if self.augmentation and self.mode == 'train':\n",
    "            centers = self.__augment__(centers)\n",
    "            corners = self.__augment__(corners)\n",
    "\n",
    "        # make corner relative to center\n",
    "        corners = corners - centers[:, np.newaxis, :]\n",
    "        corners = corners.reshape([-1, 9])\n",
    "\n",
    "        # to tensor\n",
    "        centers = torch.from_numpy(centers).float().permute(1, 0).contiguous()\n",
    "        corners = torch.from_numpy(corners).float().permute(1, 0).contiguous()\n",
    "        normals = torch.from_numpy(normals).float().permute(1, 0).contiguous()\n",
    "        neighbors_index = torch.from_numpy(neighbors_index).long()\n",
    "\n",
    "        return torch_image, centers, corners, normals, neighbors_index\n",
    "\n",
    "    def __augment__(self, data):\n",
    "        sigma, clip = 0.01, 0.05\n",
    "        jittered_data = np.clip(sigma * np.random.randn(*data.shape), -clip, clip)\n",
    "        return data + jittered_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def close(self):\n",
    "        self.renderer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(torch.jit.ScriptModule):\n",
    "    r\"\"\"A placeholder identity operator that is argument-insensitive.\n",
    "     Args:\n",
    "        args: any argument (unused)\n",
    "        kwargs: any keyword argument (unused)\n",
    "     Examples::\n",
    "         >>> m = nn.Identity(54, unused_argumenbt1=0.1, unused_argument2=False)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 20])\n",
    "     \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    @torch.jit.script_method\n",
    "    def forward(self, input):\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image2ComEmbedding(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Image2ComEmbedding, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        resnet.fc = Identity()\n",
    "        self.resnet = resnet\n",
    "        \n",
    "        self.embed_resnet = torch.nn.Sequential(OrderedDict([\n",
    "            ('embed_resnet_fc1', torch.nn.Linear(2048, 512)),\n",
    "            ('embed_resnet_relu1', torch.nn.ReLU6()),\n",
    "            ('embed_resnet_dropout', torch.nn.Dropout(p=0.5)),\n",
    "            ('embed_resnet_fc2', torch.nn.Linear(512, 1024)),\n",
    "            ('embed_resnet_relu2', torch.nn.ReLU6()),\n",
    "        ]))\n",
    "\n",
    "        if not cfg['refine']:\n",
    "            for pmt in self.resnet.parameters():\n",
    "                pmt.requires_grad = False\n",
    "                \n",
    "    def forward(self, images):\n",
    "        resnet_feature = self.resnet(images)\n",
    "        embedded_image = self.embed_resnet(resnet_feature)\n",
    "        \n",
    "        return embedded_image\n",
    "    \n",
    "class Model3D2ComEmbedding(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Model3D2ComEmbedding, self).__init__()\n",
    "        self.meshnet = MeshNet(cfg, head=False)\n",
    "        pretrained = cfg.get('pretrained', None)\n",
    "        if pretrained:\n",
    "            state_dict = torch.load(pretrained, \n",
    "                map_location=lambda storage, location: storage.cuda() if torch.cuda.is_available() else storage)\n",
    "            self.meshnet.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "        self.embed_meshnet = torch.nn.Sequential(OrderedDict([\n",
    "            ('embed_meshnet_fc1', torch.nn.Linear(256, 1280)),\n",
    "            ('embed_resnet_relu1', torch.nn.ReLU6()),\n",
    "            ('embed_meshnet_dropout', torch.nn.Dropout(p=0.5)),\n",
    "            ('embed_meshnet_fc2', torch.nn.Linear(1280, 1024)),\n",
    "            ('embed_resnet_relu2', torch.nn.ReLU6()),\n",
    "        ]))\n",
    "        \n",
    "        if not cfg['refine']:\n",
    "            for pmt in self.meshnet.parameters():\n",
    "                pmt.requires_grad = False\n",
    "            \n",
    "    def forward(self, centers, corners, normals, neighbor_index):        \n",
    "        meshnet_feature = self.meshnet(centers, corners, normals, neighbor_index)\n",
    "        embedded_3dmodel = self.embed_meshnet(meshnet_feature)\n",
    "        \n",
    "        return embedded_3dmodel\n",
    "    \n",
    "class IMEmbedding(torch.jit.ScriptModule):\n",
    "    def __init__(self, cfg):\n",
    "        super(IMEmbedding, self).__init__()\n",
    "        batch_size = cfg['batch_size']\n",
    "        image_size = cfg['image_size']\n",
    "        model3d_size = cfg['model3d_size']\n",
    "        \n",
    "        embed_image = Image2ComEmbedding(cfg['resnet'])\n",
    "        self.embed_image = embed_image\n",
    "        images = torch.randn(1, 3, image_size, image_size, dtype=torch.float32)\n",
    "        embed_image.eval()\n",
    "        self.embed_image_eval = torch.jit.trace(embed_image, [images], check_trace=False)\n",
    "        embed_image.train()\n",
    "        self.embed_image_train = torch.jit.trace(embed_image, [images], check_trace=False)\n",
    "        \n",
    "        embed_model3d = Model3D2ComEmbedding(cfg['meshnet'])\n",
    "        self.embed_model3d = embed_model3d\n",
    "        centers = torch.randn(1, 3, model3d_size, dtype=torch.float32)\n",
    "        normals = torch.randn(1, 3, model3d_size, dtype=torch.float32)\n",
    "        corners = torch.randn(1, 9, model3d_size, dtype=torch.float32)\n",
    "        neighbor_index = torch.randint(0, model3d_size, (1, model3d_size, 3), dtype=torch.long)\n",
    "        embed_model3d.eval()\n",
    "        self.embed_model3d_eval = torch.jit.trace(embed_model3d, \n",
    "                                  [centers, corners, normals, neighbor_index], check_trace=False)\n",
    "        embed_model3d.train()\n",
    "        self.embed_model3d_train = torch.jit.trace(embed_model3d, \n",
    "                                   [centers, corners, normals, neighbor_index], check_trace=False)\n",
    "        \n",
    "        \n",
    "    @torch.jit.script_method\n",
    "    def __loss(self, images, model3d):\n",
    "        # [image, model3d] or [batch, batch]\n",
    "        im_similarity = torch.matmul(images, model3d.transpose(0, 1)) \n",
    "        # ground-truth\n",
    "        sim_gt = torch.diagonal(im_similarity)\n",
    "        # curremt match\n",
    "        sorted_image, sarg_image = torch.sort(im_similarity, dim=1, descending=True)\n",
    "        sorted_model3d, sarg_model3d = torch.sort(im_similarity, dim=0, descending=True)\n",
    "        \n",
    "        top1_image = sorted_image[:, 0]\n",
    "        top1_model3d = sorted_model3d[0, :]\n",
    "        \n",
    "        # loss weights\n",
    "        batch_size = im_similarity.shape[0]\n",
    "        range_batch = torch.arange(0, batch_size, dtype=torch.long)\n",
    "        grid_image, grid_model3d = torch.meshgrid(range_batch, range_batch)\n",
    "        gt_image_rank = torch.nonzero(sarg_image == grid_image)[:, 1]\n",
    "        gt_model3d_rank = torch.nonzero(sarg_model3d == grid_model3d)[:, 0]\n",
    "        \n",
    "        # combine loss with weights\n",
    "        loss_image = torch.dot((top1_image - sim_gt), gt_image_rank.float() / batch_size)\n",
    "        loss_model3d = torch.dot((top1_model3d - sim_gt), gt_model3d_rank.float() / batch_size)\n",
    "        loss = (torch.sum(loss_image) + torch.sum(loss_model3d)) / batch_size\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def forward(self, images, centers, corners, normals, neighbor_index):\n",
    "        embedded_images = self.embed_image_train(images)\n",
    "        embedded_model3d = self.embed_model3d_train(centers, corners, normals, neighbor_index)\n",
    "        \n",
    "        return embedded_images, embedded_model3d\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def forward_with_loss(self, images, centers, corners, normals, neighbor_index):\n",
    "        embedded_images, embedded_model3d = self.forward(images, centers, corners, normals, neighbor_index)\n",
    "        loss = self.__loss(embedded_images, embedded_model3d)\n",
    "        return loss\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def forward_eval(self, images, centers, corners, normals, neighbor_index):\n",
    "        embedded_images = self.embed_image_eval(images)\n",
    "        embedded_model3d = self.embed_model3d_eval(centers, corners, normals, neighbor_index)\n",
    "        \n",
    "        return embedded_images, embedded_model3d\n",
    "    \n",
    "    @torch.jit.script_method\n",
    "    def forward_with_loss_eval(self, images, centers, corners, normals, neighbor_index):\n",
    "        embedded_images, embedded_model3d = self.forward_eval(images, centers, corners, normals, neighbor_index)\n",
    "        loss = self.__loss(embedded_images, embedded_model3d)\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, dataloaders, optimizer=None, start_epoch=1, end_epochs=30, ckpt_root='ckpt_root'):\n",
    "        if optimizer is None:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        \n",
    "        val_loss_hist = []\n",
    "        for epoch in range(start_epoch, end_epochs + 1):\n",
    "            print('-' * 60)\n",
    "            print('Epoch: {} / {}'.format(epoch, end_epochs))\n",
    "            print('-' * 60)\n",
    "            \n",
    "            for phrase in ['train', 'val']:\n",
    "                if phrase == 'train':\n",
    "                    self.train()\n",
    "                else:\n",
    "                    self.eval()\n",
    "\n",
    "                running_loss = 0.0\n",
    "                batch_size = dataloaders[phrase].batch_size\n",
    "                dataset_size = len(dataloaders[phrase].dataset)\n",
    "                total_steps = int(dataset_size / batch_size)\n",
    "                \n",
    "                for step, (images, centers, corners, normals, neighbor_index) in enumerate(dataloaders[phrase]):\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    if torch.cuda.is_available():\n",
    "                        images = images.cuda()\n",
    "                        centers = centers.cuda()\n",
    "                        normals = normals.cuda()\n",
    "                        corners = corners.cuda()\n",
    "                        neighbor_index = neighbor_index.cuda()\n",
    "                        targets = targets.cuda()\n",
    "                        \n",
    "                    with torch.set_grad_enabled(phrase == 'train'):\n",
    "                        \n",
    "                        \n",
    "                        if phrase == 'train':\n",
    "                            loss = self.forward_with_loss(images, centers, corners, normals, neighbor_index)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                        else:\n",
    "                            loss = self.forward_with_loss_eval(images, centers, corners, normals, neighbor_index)\n",
    "                            \n",
    "                        batch_loss = loss.item()\n",
    "                        running_loss += batch_loss * batch_size\n",
    "                        \n",
    "                    print(f'{datetime.now()} {phrase} epoch: {epoch}/{end_epochs} '\n",
    "                          f'step:{step + 1}/{total_steps} loss: {batch_loss:.4f}')\n",
    "                          \n",
    "                epoch_loss = running_loss / dataset_size\n",
    "                print(f'{phrase} epoch: {epoch}/{end_epochs} loss: {epoch_loss:.4f}')\n",
    "                \n",
    "                if phrase == 'train':\n",
    "                    filename = os.path.join(ckpt_root, f'{epoch:04d}.pkl')\n",
    "                    torch.save(copy.deepcopy(self.state_dict()), filename)\n",
    "                \n",
    "                if phrase == 'val':\n",
    "                    val_loss_hist.append(epoch_loss)\n",
    "                    \n",
    "        return val_loss_hist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'resnet': {\n",
    "        'refine': False\n",
    "    },\n",
    "    'meshnet': {\n",
    "        'refine': False,\n",
    "        'structural_descriptor': {\n",
    "            'num_kernel': 64,\n",
    "            'sigma': 0.2\n",
    "        },\n",
    "        'mesh_convolution': {\n",
    "            'aggregation_method': 'Concat'\n",
    "        },\n",
    "        'pretrained': './data/meshnet.pkl'\n",
    "    },\n",
    "    'batch_size': 4,\n",
    "    'image_size': 224,\n",
    "    'model3d_size': 1024,\n",
    "    'data_root': './data/ModelNet40',\n",
    "    'augmentation': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = object3d.Panda3DRenderer(output_size=(224, 224), \n",
    "                cast_shadow=True, light_on=True)\n",
    "loader_dict = {\n",
    "    'train': torch.utils.data.DataLoader(ModelNet40WithImage(cfg, renderer, mode='train'), 4, num_workers=0,\n",
    "                                         shuffle=True, pin_memory=True, drop_last=True),\n",
    "    'val': torch.utils.data.DataLoader(ModelNet40WithImage(cfg, renderer, mode='test'), 4, num_workers=0,\n",
    "                                         shuffle=True, pin_memory=True, drop_last=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = IMEmbedding(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Epoch: 1 / 30\n",
      "------------------------------------------------------------\n",
      "2019-04-30 18:45:25.845807 train epoch: 1/30 step:1/2460 loss: 14.2821\n",
      "2019-04-30 18:45:32.353376 train epoch: 1/30 step:2/2460 loss: 10.8488\n",
      "2019-04-30 18:45:38.947149 train epoch: 1/30 step:3/2460 loss: 22.9048\n",
      "2019-04-30 18:45:47.431597 train epoch: 1/30 step:4/2460 loss: 8.4489\n",
      "2019-04-30 18:45:53.719084 train epoch: 1/30 step:5/2460 loss: 16.5312\n",
      "2019-04-30 18:46:00.358389 train epoch: 1/30 step:6/2460 loss: 13.9870\n",
      "2019-04-30 18:46:07.237415 train epoch: 1/30 step:7/2460 loss: 7.9650\n",
      "2019-04-30 18:46:13.551401 train epoch: 1/30 step:8/2460 loss: 17.9350\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-9117420a9132>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-6d4b79d09bbf>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataloaders, optimizer, start_epoch, end_epochs, ckpt_root)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mphrase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_with_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbor_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m                             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding.fit(loader_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = loader_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.batch_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
